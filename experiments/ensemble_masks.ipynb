{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import torch\n",
    "from monai.networks.nets import UNet as monai_unet\n",
    "from monai.networks.nets import AttentionUnet as monai_att\n",
    "from semantic_bac_segment.models.pytorch_attention import AttentionUNet as base_att\n",
    "from monai.networks.nets import UNETR as monai_unetr\n",
    "from semantic_bac_segment.models.pytorch_cnnunet import Unet as atomai_unet\n",
    "from semantic_bac_segment.segmentator import Segmentator3\n",
    "from tifffile import imread\n",
    "\n",
    "# Set the paths to the results folder and the source images\n",
    "results_folder = '../results/'\n",
    "source_images_folder = '../data/source_norm/'\n",
    "\n",
    "# Initialize an empty list to store the prediction stacks\n",
    "prediction_stacks = []\n",
    "\n",
    "images=glob.glob(os.path.join(results_folder, '*.tiff'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from semantic_bac_segment.utils import get_device, normalize_percentile, empty_gpu_cache\n",
    "from monai.inferers import SlidingWindowInferer\n",
    "import gc\n",
    "\n",
    "class Segmentator4:\n",
    "    \"\"\"\n",
    "    A class representing a segmentation model.\n",
    "\n",
    "    Attributes:\n",
    "        model (torch.nn.Module): The segmentation model.\n",
    "        device (torch.device): The device on which the model is loaded.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_path, model_graph, patch_size, overlap_ratio, half_precision=False):\n",
    "        \"\"\"\n",
    "        Initializes a Segmentator object.\n",
    "\n",
    "        Args:\n",
    "            model_path (str): The path to the model.\n",
    "            model_graph (str): The model graph.\n",
    "            patch_size (int): The size of the patches.\n",
    "            overlap_ratio (float): The overlap ratio between patches.\n",
    "        \"\"\"\n",
    "        self.device = get_device()\n",
    "        self.model = self.get_model(model_path, self.device, model_graph=model_graph)\n",
    "        self.patch_size = patch_size\n",
    "        self.overlap_ratio = overlap_ratio\n",
    "        self.model.eval()\n",
    "        self.half_precision=half_precision\n",
    "        if self.half_precision:\n",
    "            self.model.half()  \n",
    "\n",
    "\n",
    "    def predict(self, image):\n",
    "        \"\"\"\n",
    "        Predicts the segmentation mask for the given image. It can handle 2D images or a stack of 2D images.\n",
    "\n",
    "        Args:\n",
    "            image (numpy.ndarray): The input image or image stack.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: The segmentation mask or stack of segmentation masks.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Normalize image\n",
    "        image = normalize_percentile(image)\n",
    "\n",
    "        # Check if the image is a stack\n",
    "        if len(image.shape) > 2:\n",
    "            # Store the original image size and number of slices\n",
    "            original_size = image.shape[1:]\n",
    "            num_slices = image.shape[0]\n",
    "\n",
    "            # Convert the entire stack to a tensor and add batch dimension\n",
    "            img_tensor = torch.from_numpy(image).unsqueeze(1).to(self.device)  # Shape: [1, C, H, W]\n",
    "            if self.half_precision:\n",
    "                img_tensor = img_tensor.half()  # Convert input to half-precision\n",
    "\n",
    "            # Create SlidingWindowInferer\n",
    "            inferer = SlidingWindowInferer(roi_size=self.patch_size, sw_batch_size=1, overlap=self.overlap_ratio)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output_mask = inferer(img_tensor, self.model)\n",
    "\n",
    "            output_mask = output_mask.cpu().numpy()  # Remove batch dimension\n",
    "\n",
    "            # Split the result back into the original chunks\n",
    "            #output_mask = np.dsplit(output_mask, num_slices)\n",
    "\n",
    "        else:\n",
    "            # Process the image as before\n",
    "            img_tensor = torch.from_numpy(image).unsqueeze(0).unsqueeze(0).to(self.device)\n",
    "\n",
    "            if self.half_precision:\n",
    "                img_tensor = img_tensor.half()  # Convert input to half-precision\n",
    "\n",
    "            # Create SlidingWindowInferer\n",
    "            inferer = SlidingWindowInferer(roi_size=self.patch_size, sw_batch_size=350, overlap=self.overlap_ratio)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output_mask = inferer(img_tensor, self.model)\n",
    "\n",
    "            output_mask = output_mask.cpu().numpy()\n",
    "\n",
    "            # Free up tensors\n",
    "#            del img_tensor, image  \n",
    "#            gc.collect() \n",
    "#            empty_gpu_cache(self.device)\n",
    "        print(output_mask.shape)\n",
    "        return output_mask\n",
    "   \n",
    "\n",
    "    def get_model(self, path, device, model_graph=None):\n",
    "        \"\"\"\n",
    "        Loads a model from the specified path and returns it.\n",
    "\n",
    "        Args:\n",
    "            path (str): The path to the model file.\n",
    "            device (str): The device to load the model onto.\n",
    "            model_graph (Optional[torch.nn.Module]): An optional pre-initialized model graph.\n",
    "\n",
    "        Returns:\n",
    "            torch.nn.Module: The loaded model.\n",
    "\n",
    "        Raises:\n",
    "            FileNotFoundError: If the model file is not found at the specified path.\n",
    "            RuntimeError: If an error occurs while loading the model.\n",
    "            Exception: If an unexpected error occurs.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if model_graph is None:\n",
    "                model = torch.load(path, map_location=device)\n",
    "            else:\n",
    "                model = model_graph\n",
    "                state_dict = torch.load(path, map_location=device)\n",
    "                \n",
    "                # Check if the loaded state dictionary is compatible with the model architecture\n",
    "                if not set(state_dict.keys()).issubset(set(model.state_dict().keys())):\n",
    "                    raise ValueError(\"Loaded state dictionary does not match the model architecture.\")\n",
    "                \n",
    "                model.load_state_dict(state_dict)\n",
    "            \n",
    "            model.to(device)\n",
    "            torch.compile(model, mode = 'max-autotune')\n",
    "            \n",
    "            return model\n",
    "        \n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError(f\"Model file not found at path: {path}\")\n",
    "        \n",
    "        except RuntimeError as e:\n",
    "            raise RuntimeError(f\"Error occurred while loading the model: {str(e)}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Unexpected error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dict = []\n",
    "\n",
    "for file in os.listdir(results_folder):\n",
    "    if file.endswith('_model.pth'):\n",
    "        model_path = os.path.join(results_folder, file)\n",
    "        config_path = os.path.join(results_folder, file.replace('_model.pth', '_config.json'))\n",
    "\n",
    "        # Check if the corresponding config file exists\n",
    "        if os.path.exists(config_path):\n",
    "            # Add the model and its configuration to the dictionary\n",
    "            models_dict.append((model_path, config_path))\n",
    "        else:\n",
    "            print(f\"Warning: Config file not found for model {file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('../results/unet_monai_best_model.pth',\n",
       "  '../results/unet_monai_best_config.json'),\n",
       " ('../results/AttentionUNet-2_best_model.pth',\n",
       "  '../results/AttentionUNet-2_best_config.json'),\n",
       " ('../results/MonaiUnet-1_best_model.pth',\n",
       "  '../results/MonaiUnet-1_best_config.json'),\n",
       " ('../results/MonaiUnet-2_final_model.pth',\n",
       "  '../results/MonaiUnet-2_final_config.json'),\n",
       " ('../results/atomai_unet-8_best_model.pth',\n",
       "  '../results/atomai_unet-8_best_config.json'),\n",
       " ('../results/MonaiUnet-2_best_model.pth',\n",
       "  '../results/MonaiUnet-2_best_config.json'),\n",
       " ('../results/unet_monai_best2_model.pth',\n",
       "  '../results/unet_monai_best2_config.json'),\n",
       " ('../results/unet_model_best-binary2-channel0_model.pth',\n",
       "  '../results/unet_model_best-binary2-channel0_config.json'),\n",
       " ('../results/MonaiUnet-1_final_model.pth',\n",
       "  '../results/MonaiUnet-1_final_config.json'),\n",
       " ('../results/AHNet-8_best_model.pth', '../results/AHNet-8_best_config.json')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../results/unet_monai_best_model.pth\n",
      "../results/unet_monai_best_config.json\n",
      "../results/AttentionUNet-2_best_model.pth\n",
      "../results/AttentionUNet-2_best_config.json\n",
      "../results/MonaiUnet-1_best_model.pth\n",
      "../results/MonaiUnet-1_best_config.json\n",
      "../results/MonaiUnet-2_final_model.pth\n",
      "../results/MonaiUnet-2_final_config.json\n",
      "../results/atomai_unet-8_best_model.pth\n",
      "../results/atomai_unet-8_best_config.json\n",
      "../results/MonaiUnet-2_best_model.pth\n",
      "../results/MonaiUnet-2_best_config.json\n",
      "../results/unet_monai_best2_model.pth\n",
      "../results/unet_monai_best2_config.json\n",
      "../results/unet_model_best-binary2-channel0_model.pth\n",
      "../results/unet_model_best-binary2-channel0_config.json\n",
      "../results/MonaiUnet-1_final_model.pth\n",
      "../results/MonaiUnet-1_final_config.json\n",
      "../results/AHNet-8_best_model.pth\n",
      "../results/AHNet-8_best_config.json\n"
     ]
    }
   ],
   "source": [
    "for model_path, config_file in models_dict:\n",
    "    print(model_path)\n",
    "    print(config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../results/unet_monai_best_model.pth with config ../results/unet_monai_best_config.json\n",
      "Using monai_unet architecture for ../results/unet_monai_best_model.pth\n",
      "(1, 3, 2400, 2400)\n",
      "(1, 3, 2400, 2400)\n",
      "(1, 3, 2400, 2400)\n",
      "(1, 3, 2400, 2400)\n",
      "(1, 3, 2400, 2400)\n",
      "(1, 3, 2400, 2400)\n",
      "(1, 3, 2400, 2400)\n",
      "(1, 3, 2400, 2400)\n",
      "(1, 3, 2400, 2400)\n",
      "(1, 3, 2400, 2400)\n",
      "(1, 3, 2400, 2400)\n",
      "(1, 3, 2400, 2400)\n",
      "(1, 3, 2400, 2400)\n",
      "(1, 3, 2400, 2400)\n",
      "(1, 3, 2400, 2400)\n",
      "(1, 3, 2400, 2400)\n",
      "(1, 3, 2400, 2400)\n",
      "(1, 3, 2400, 2400)\n",
      "(1, 3, 2400, 2400)\n",
      "(1, 3, 2400, 2400)\n",
      "(1, 3, 2400, 2400)\n",
      "(1, 3, 2400, 2400)\n",
      "(1, 3, 2400, 2400)\n",
      "(1, 3, 2400, 2400)\n",
      "(1, 3, 2400, 2400)\n",
      "(1, 3, 2400, 2400)\n",
      "(1, 3, 2400, 2400)\n",
      "(1, 3, 2400, 2400)\n",
      "(1, 3, 2400, 2400)\n",
      "(1, 3, 2400, 2400)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:43<06:31, 43.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3, 2400, 2400)\n",
      "Processing ../results/AttentionUNet-2_best_model.pth with config ../results/AttentionUNet-2_best_config.json\n",
      "Using base_att architecture for ../results/AttentionUNet-2_best_model.pth\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [02:25<10:23, 77.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 2400, 2400)\n",
      "Processing ../results/MonaiUnet-1_best_model.pth with config ../results/MonaiUnet-1_best_config.json\n",
      "Using monai_unet architecture for ../results/MonaiUnet-1_best_model.pth\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [03:11<07:24, 63.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 2400, 2400)\n",
      "Processing ../results/MonaiUnet-2_final_model.pth with config ../results/MonaiUnet-2_final_config.json\n",
      "Using monai_unet architecture for ../results/MonaiUnet-2_final_model.pth\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [06:15<11:05, 110.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 2400, 2400)\n",
      "Processing ../results/atomai_unet-8_best_model.pth with config ../results/atomai_unet-8_best_config.json\n",
      "Using atomai_unet architecture for ../results/atomai_unet-8_best_model.pth\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [07:51<08:47, 105.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 2400, 2400)\n",
      "Processing ../results/MonaiUnet-2_best_model.pth with config ../results/MonaiUnet-2_best_config.json\n",
      "Using monai_unet architecture for ../results/MonaiUnet-2_best_model.pth\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [10:49<08:40, 130.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 2400, 2400)\n",
      "Processing ../results/unet_monai_best2_model.pth with config ../results/unet_monai_best2_config.json\n",
      "Using monai_unet architecture for ../results/unet_monai_best2_model.pth\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [11:34<05:07, 102.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 2400, 2400)\n",
      "Processing ../results/unet_model_best-binary2-channel0_model.pth with config ../results/unet_model_best-binary2-channel0_config.json\n",
      "Using monai_unet architecture for ../results/unet_model_best-binary2-channel0_model.pth\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [12:18<02:47, 83.78s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 2400, 2400)\n",
      "Processing ../results/MonaiUnet-1_final_model.pth with config ../results/MonaiUnet-1_final_config.json\n",
      "Using monai_unet architecture for ../results/MonaiUnet-1_final_model.pth\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n",
      "(1, 1, 2400, 2400)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [13:04<01:12, 72.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 2400, 2400)\n",
      "Processing ../results/AHNet-8_best_model.pth with config ../results/AHNet-8_best_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [13:06<01:27, 87.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using monai_unetr architecture for ../results/AHNet-8_best_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "missing a required argument: 'img_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 28\u001b[0m\n\u001b[1;32m     22\u001b[0m     net_architecture \u001b[38;5;241m=\u001b[39m monai_unet\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Load the model with the configuration parameters\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m#model = net_architecture(**config['model_args'])\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Initialize the Segmentator3 with the loaded model\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m segmentator \u001b[38;5;241m=\u001b[39m Segmentator4(model_path, \u001b[43mnet_architecture\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_args\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     29\u001b[0m                             patch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m, overlap_ratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.25\u001b[39m, half_precision\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Initialize an empty list to store the predictions for the current model\u001b[39;00m\n\u001b[1;32m     32\u001b[0m model_predictions \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/miniconda3/envs/img_pytorch/lib/python3.9/site-packages/monai/utils/deprecate_utils.py:208\u001b[0m, in \u001b[0;36mdeprecated_arg.<locals>._decorator.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m         \u001b[38;5;66;03m# multiple values for new_name using both args and kwargs\u001b[39;00m\n\u001b[1;32m    207\u001b[0m         kwargs\u001b[38;5;241m.\u001b[39mpop(new_name, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 208\u001b[0m binding \u001b[38;5;241m=\u001b[39m \u001b[43msig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39marguments\n\u001b[1;32m    209\u001b[0m positional_found \u001b[38;5;241m=\u001b[39m name \u001b[38;5;129;01min\u001b[39;00m binding\n\u001b[1;32m    210\u001b[0m kw_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/img_pytorch/lib/python3.9/inspect.py:3045\u001b[0m, in \u001b[0;36mSignature.bind\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3040\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbind\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m/\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   3041\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get a BoundArguments object, that maps the passed `args`\u001b[39;00m\n\u001b[1;32m   3042\u001b[0m \u001b[38;5;124;03m    and `kwargs` to the function's signature.  Raises `TypeError`\u001b[39;00m\n\u001b[1;32m   3043\u001b[0m \u001b[38;5;124;03m    if the passed arguments can not be bound.\u001b[39;00m\n\u001b[1;32m   3044\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3045\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bind\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/img_pytorch/lib/python3.9/inspect.py:3015\u001b[0m, in \u001b[0;36mSignature._bind\u001b[0;34m(self, args, kwargs, partial)\u001b[0m\n\u001b[1;32m   3008\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m   3009\u001b[0m     \u001b[38;5;66;03m# We have no value for this parameter.  It's fine though,\u001b[39;00m\n\u001b[1;32m   3010\u001b[0m     \u001b[38;5;66;03m# if it has a default value, or it is an '*args'-like\u001b[39;00m\n\u001b[1;32m   3011\u001b[0m     \u001b[38;5;66;03m# parameter, left alone by the processing of positional\u001b[39;00m\n\u001b[1;32m   3012\u001b[0m     \u001b[38;5;66;03m# arguments.\u001b[39;00m\n\u001b[1;32m   3013\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m partial \u001b[38;5;129;01mand\u001b[39;00m param\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;241m!=\u001b[39m _VAR_POSITIONAL \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m   3014\u001b[0m                                         param\u001b[38;5;241m.\u001b[39mdefault \u001b[38;5;129;01mis\u001b[39;00m _empty):\n\u001b[0;32m-> 3015\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmissing a required argument: \u001b[39m\u001b[38;5;132;01m{arg!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39m \\\n\u001b[1;32m   3016\u001b[0m                         \u001b[38;5;28mformat\u001b[39m(arg\u001b[38;5;241m=\u001b[39mparam_name)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3018\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3019\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m param\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;241m==\u001b[39m _POSITIONAL_ONLY:\n\u001b[1;32m   3020\u001b[0m         \u001b[38;5;66;03m# This should never happen in case of a properly built\u001b[39;00m\n\u001b[1;32m   3021\u001b[0m         \u001b[38;5;66;03m# Signature object (but let's have this check here\u001b[39;00m\n\u001b[1;32m   3022\u001b[0m         \u001b[38;5;66;03m# to ensure correct behaviour just in case)\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: missing a required argument: 'img_size'"
     ]
    }
   ],
   "source": [
    "from  tqdm.auto import tqdm \n",
    "\n",
    "# Iterate over each pair of model/config in the results folder\n",
    "for model_path, config_path in tqdm(models_dict):\n",
    "        print(f'Processing {model_path} with config {config_path}')\n",
    "        # Load the model configuration\n",
    "        with open(config_path, 'r') as f:\n",
    "            config = json.load(f)\n",
    "\n",
    "        # Determine the net architecture based on the model name\n",
    "        if 'atomai' in model_path:\n",
    "            print(f'Using atomai_unet architecture for {model_path}')\n",
    "            net_architecture = atomai_unet\n",
    "        elif 'Attention' in model_path:\n",
    "            print(f'Using base_att architecture for {model_path}')\n",
    "            net_architecture = base_att\n",
    "        elif 'AHNet' in model_path:\n",
    "            print(f'Using monai_unetr architecture for {model_path}')\n",
    "            net_architecture = monai_unetr\n",
    "        else:\n",
    "            print(f'Using monai_unet architecture for {model_path}')\n",
    "            net_architecture = monai_unet\n",
    "\n",
    "        # Load the model with the configuration parameters\n",
    "        #model = net_architecture(**config['model_args'])\n",
    "\n",
    "        # Initialize the Segmentator3 with the loaded model\n",
    "        segmentator = Segmentator4(model_path, net_architecture(**config['model_args']),\n",
    "                                    patch_size=256, overlap_ratio=0.25, half_precision=True)\n",
    "        \n",
    "        # Initialize an empty list to store the predictions for the current model\n",
    "        model_predictions = []\n",
    "\n",
    "        # Iterate over each source image\n",
    "        for image_file in os.listdir(source_images_folder):\n",
    "            if image_file.endswith('.tiff'):\n",
    "                image_path = os.path.join(source_images_folder, image_file)\n",
    "                image = imread(image_path)\n",
    "\n",
    "                # Get predictions\n",
    "                prediction = segmentator.predict(image)\n",
    "                prediction=sigmoid(prediction)\n",
    "                model_predictions.append(prediction)\n",
    "\n",
    "        prediction_stacks.append(model_predictions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l9/mzf3xz016nn449t2nlqp0k680000gn/T/ipykernel_75221/638571445.py:4: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n"
     ]
    }
   ],
   "source": [
    "arr_list=[]\n",
    "for i in prediction_stacks:\n",
    "    arr_list.append(np.array(i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_list\n",
    "for i in range(len(arr_list)):\n",
    "    arr_list[i]=np.sum(arr_list[i], axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31, 1, 2400, 2400)\n",
      "(31, 1, 2400, 2400)\n",
      "(31, 1, 2400, 2400)\n",
      "(31, 1, 2400, 2400)\n",
      "(31, 1, 2400, 2400)\n",
      "(31, 1, 2400, 2400)\n",
      "(31, 1, 2400, 2400)\n",
      "(31, 1, 2400, 2400)\n",
      "(31, 1, 2400, 2400)\n",
      "(31, 1, 2400, 2400)\n",
      "(31, 1, 2400, 2400)\n"
     ]
    }
   ],
   "source": [
    "arr_list\n",
    "for i in range(len(arr_list)):\n",
    "    print(arr_list[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_arrays = np.stack(arr_list, axis=0)\n",
    "\n",
    "# Take the mean along the new axis (axis=0)\n",
    "averaged_array = np.mean(stacked_arrays, axis=(0, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.1594 , 0.1362 , 0.12146, ..., 1.377  , 1.319  , 1.239  ],\n",
       "       [0.1335 , 0.1042 , 0.1019 , ..., 1.403  , 1.391  , 1.309  ],\n",
       "       [0.11926, 0.1024 , 0.0995 , ..., 1.412  , 1.407  , 1.332  ],\n",
       "       ...,\n",
       "       [0.1129 , 0.0903 , 0.087  , ..., 1.3545 , 1.331  , 1.286  ],\n",
       "       [0.1129 , 0.1003 , 0.09204, ..., 1.328  , 1.305  , 1.25   ],\n",
       "       [0.1339 , 0.1148 , 0.1081 , ..., 1.272  , 1.242  , 1.15   ]],\n",
       "      dtype=float16)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "averaged_array[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tifffile\n",
    "tifffile.imwrite(os.path.join('../data/average_pred.tiff'), averaged_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coli_mask_frame_205\n",
      "mabs_img_13\n",
      "mabs_img_4\n",
      "mabs_img_8\n",
      "coli_mask_frame_1\n",
      "coli_mask_frame_232\n",
      "mabs_img_9\n",
      "coli_mask_frame_101\n",
      "mabs_img_5\n",
      "mabs_img_12\n",
      "mabs_img_19\n",
      "coli_mask_frame_223\n",
      "coli_mask_frame_274\n",
      "mabs_img_15\n",
      "mabs_img_2\n",
      "mabs_img_3\n",
      "mabs_img_14\n",
      "coli_mask_frame_166\n",
      "mabs_img_18\n",
      "coli_mask_frame_10\n",
      "coli_mask_frame_165\n",
      "mabs_img_17\n",
      "mabs_img_16\n",
      "mabs_img_1\n",
      "mabs_img_20\n",
      "coli_mask_frame_109\n",
      "mabs_img_6\n",
      "mabs_img_11\n",
      "coli_mask_frame_56\n",
      "mabs_img_10\n",
      "mabs_img_7\n"
     ]
    }
   ],
   "source": [
    "image_titles = []\n",
    "\n",
    "# Iterate over the image files and extract their names as titles\n",
    "for image_file in os.listdir(source_images_folder):\n",
    "    # Extract the image title from the file name\n",
    "    image_title = os.path.splitext(image_file)[0]\n",
    "    image_title = os.path.splitext(image_file)[0]\n",
    "    if '.DS_Store' in image_title:\n",
    "        continue\n",
    "\n",
    "    print(image_title)\n",
    "    image_titles.append(image_title)\n",
    "\n",
    "# Create a dictionary to store the metadata\n",
    "imagej_metadata = {\n",
    "    'Labels': image_titles\n",
    "}\n",
    "tifffile.imwrite(os.path.join('../data/average_pred.tiff'), averaged_array.astype(np.float32), metadata=imagej_metadata, imagej=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tifffile\n",
    "sum_array = np.sum(stacked_arrays, axis=(0, 2))\n",
    "tifffile.imwrite(os.path.join('../data/summed_pred.tiff'), sum_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tifffile\n",
    "\n",
    "# ... (Your existing code to generate averaged_array) ...\n",
    "\n",
    "# Convert averaged_array to a supported data type (e.g., float32)\n",
    "averaged_array = averaged_array.astype(np.float32)\n",
    "\n",
    "# Create a list to store the image titles and corresponding slices\n",
    "image_data = []\n",
    "\n",
    "# Iterate over the image files and extract their names as titles\n",
    "for image_file in os.listdir(source_images_folder):\n",
    "    # Extract the image title from the file name\n",
    "    image_title = os.path.splitext(image_file)[0]\n",
    "    if '.DS_Store' in image_title:\n",
    "        continue\n",
    "    # Assuming the slices are stored in averaged_array in the same order as the image files\n",
    "    slice_index = len(image_data)\n",
    "    \n",
    "    # Append the image title and slice index to the image_data list\n",
    "    image_data.append((image_title, slice_index))\n",
    "\n",
    "# Sort the image_data list based on the image titles\n",
    "image_data.sort(key=lambda x: x[0])\n",
    "\n",
    "# Create lists to store the sorted image titles and slices\n",
    "sorted_titles = [data[0] for data in image_data]\n",
    "sorted_slices = [averaged_array[data[1]] for data in image_data]\n",
    "\n",
    "# Stack the sorted slices to create the final sorted array\n",
    "sorted_array = np.stack(sorted_slices)\n",
    "\n",
    "# Create a dictionary to store the ImageJ metadata\n",
    "imagej_metadata = {\n",
    "    'Labels': sorted_titles\n",
    "}\n",
    "\n",
    "# Save the sorted array as a TIFF file with ImageJ metadata\n",
    "tifffile.imwrite(os.path.join('../data/average_pred_sorted.tiff'), sorted_array, imagej=True, metadata=imagej_metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "img_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
