{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import cv2\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "from pathlib import Path\n",
    "from typing import Tuple, List\n",
    "from skimage.util import view_as_windows\n",
    "class ImageAdapter:\n",
    "\n",
    "    def __init__(self, \n",
    "                 img: np.ndarray, \n",
    "                 patch_size: int, \n",
    "                 overlap_ratio: float) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the ImageAdapter class. Adapt images to the U-net input and\n",
    "        allow to stich them back together to the original shape.\n",
    "\n",
    "        Args:\n",
    "            img (np.ndarray): Input image array.\n",
    "            patch_size (int): Width and height of square patches.\n",
    "            overlap_ratio (float): Fraction of pixels to overlap.\n",
    "        \"\"\"\n",
    "        self.img = img\n",
    "        self.source_shape = img.shape\n",
    "        self.patch_size = patch_size\n",
    "        self.overlap_size = int(patch_size * overlap_ratio)\n",
    "\n",
    "    def create_patches(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Split the image into patches using view_as_windows.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Array of image patches.\n",
    "        \"\"\"\n",
    "\n",
    "        step_size = (self.patch_size - self.overlap_size, self.patch_size - self.overlap_size)\n",
    "        img = self.fit_image(self.img, step_size)\n",
    "        image_patches = view_as_windows(img, (self.patch_size, self.patch_size), step_size)\n",
    "\n",
    "        # Flatten into patches\n",
    "        self.n_patches_w = image_patches.shape[1]\n",
    "        self.n_patches_h = image_patches.shape[0]\n",
    "        image_patches = image_patches.reshape(-1, self.patch_size, self.patch_size)\n",
    "\n",
    "        return image_patches\n",
    "\n",
    "    def stich_patches(self, image_patches: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Stitch the image patches back into a single image.\n",
    "\n",
    "        Args:\n",
    "            image_patches (np.ndarray): Array of image patches.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Reconstructed image array.\n",
    "        \"\"\"\n",
    "        \n",
    "        overlap_size = self.overlap_size\n",
    "        \n",
    "        reconstructed = np.zeros((self.n_patches_h * (self.patch_size - overlap_size) + self.overlap_size,\n",
    "                                  self.n_patches_w * (self.patch_size - overlap_size) + self.overlap_size))\n",
    "        reconstructed_slices = [np.zeros_like(reconstructed) for _ in range(len(image_patches))]\n",
    "\n",
    "\n",
    "        idx_table = product(range(self.n_patches_h), range(self.n_patches_w))\n",
    "\n",
    "        for n, (i, j) in enumerate(idx_table):\n",
    "            patch = image_patches[n, :, :]\n",
    "            h_idx = i * (self.patch_size - overlap_size)\n",
    "            w_idx = j * (self.patch_size - overlap_size)\n",
    "            reconstructed_slices[n][h_idx:h_idx+self.patch_size, w_idx:w_idx+self.patch_size] = patch\n",
    "\n",
    "        reconstructed_slices=np.asarray(reconstructed_slices)\n",
    "        reconstructed = np.max(reconstructed_slices, axis=0)\n",
    "\n",
    "        # Crop the reconstructed image to the original size\n",
    "        reconstructed = reconstructed[:self.source_shape[0], :self.source_shape[1]]\n",
    "\n",
    "        return reconstructed\n",
    "\n",
    "    def fit_image(self, img: np.ndarray, step_size: Tuple[int, int]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Pad the image to ensure it is divisible by the effective patch size.\n",
    "\n",
    "        Args:\n",
    "            img (np.ndarray): Input image array.\n",
    "            step_size (Tuple[int, int]): Step size for the sliding window.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Padded image array.\n",
    "        \"\"\"\n",
    "        # Calculate the padding dims\n",
    "        remainder_h = img.shape[0] % step_size[0]\n",
    "        remainder_w = img.shape[1] % step_size[1]\n",
    "\n",
    "        pad_h = 2*step_size[0] - remainder_h if remainder_h else 0\n",
    "        pad_w = 2*step_size[1] - remainder_w if remainder_w else 0\n",
    "\n",
    "        # Pad the image\n",
    "        img = np.pad(img, ((0, pad_h), (0, pad_w)), \n",
    "                     mode='symmetric')\n",
    "\n",
    "        return img\n",
    "    \n",
    "    def clear_background(self, sigma_r=25, method='divide', convert_32=True):\n",
    "        \n",
    "        # Input checks\n",
    "        img = self.img.copy()\n",
    "\n",
    "        if img.ndim != 2:\n",
    "            raise ValueError(\"Input image must be 2D\")\n",
    "        if convert_32:\n",
    "            img = img.astype(np.float32)\n",
    "        def round_to_odd(number):\n",
    "            return int(number) if number % 2 == 1 else int(number) + 1\n",
    "        \n",
    "        # Gaussian blur\n",
    "        sigma_r=round_to_odd(sigma_r)\n",
    "        gaussian_blur = cv2.GaussianBlur(img, (sigma_r, sigma_r), 0)\n",
    "\n",
    "        # Background remove\n",
    "        if method == 'subtract':\n",
    "            background_removed = cv2.subtract(img, gaussian_blur)\n",
    "        elif method == 'divide':\n",
    "            background_removed = cv2.divide(img, gaussian_blur)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid method. Choose either 'subtract' or 'divide'\")\n",
    "        \n",
    "        self.img = background_removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_percentile(x, pmin=1, pmax=99.8, clip=False, dtype=np.float32):\n",
    "    \"\"\"\n",
    "    Percentile-based image normalization.\n",
    "\n",
    "    Args:\n",
    "        x (numpy.ndarray): Input array.\n",
    "        pmin (float): Lower percentile value (default: 1).\n",
    "        pmax (float): Upper percentile value (default: 99.8).\n",
    "        clip (bool): Whether to clip the output values to the range [0, 1] (default: False).\n",
    "        dtype (numpy.dtype): Output data type (default: np.float32).\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Normalized array.\n",
    "    \"\"\"\n",
    "\n",
    "    x = x.astype(dtype, copy=False)\n",
    "    mi = np.percentile(x, pmin)\n",
    "    ma = np.percentile(x, pmax)\n",
    "    eps = np.finfo(dtype).eps  # Get the smallest positive value for the data type\n",
    "\n",
    "    x = (x - mi) / (ma - mi + eps)\n",
    "\n",
    "    if clip:\n",
    "        x = np.clip(x, 0, 1)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tifffile\n",
    "\n",
    "img = tifffile.imread('../data/train/source/mabs_img_1.tif')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class UNET(nn.Module):\n",
    "    def __init__(\n",
    "            self, in_channels=1, out_channels=1, features=[64, 128, 256, 512], init_features=64, pooling_steps=4\n",
    "    ):\n",
    "        super(UNET, self).__init__()\n",
    "\n",
    "        if features == None:        \n",
    "            features = [2**i for i in range(pooling_steps) if 2**i >= init_features]\n",
    "        else:\n",
    "            features = features\n",
    "        self.ups = nn.ModuleList()\n",
    "        self.downs = nn.ModuleList()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Down part of UNET\n",
    "        for feature in features:\n",
    "            self.downs.append(DoubleConv(in_channels, feature))\n",
    "            in_channels = feature\n",
    "\n",
    "        # Up part of UNET\n",
    "        for feature in reversed(features):\n",
    "            self.ups.append(\n",
    "                nn.ConvTranspose2d(\n",
    "                    feature*2, feature, kernel_size=2, stride=2,\n",
    "                )\n",
    "            )\n",
    "            self.ups.append(DoubleConv(feature*2, feature))\n",
    "\n",
    "        self.bottleneck = DoubleConv(features[-1], features[-1]*2)\n",
    "        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n",
    "\n",
    "\n",
    "    def load_weights(self, weights_path):\n",
    "        \"\"\"\n",
    "        Load weights from a previously trained model.\n",
    "\n",
    "        Args:\n",
    "            weights_path (str): Path to the weights file.\n",
    "        \"\"\"\n",
    "        self.load_state_dict(torch.load(weights_path))\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, weights_path, **kwargs):\n",
    "        \"\"\"\n",
    "        Create a UNet model and initialize it with pretrained weights.\n",
    "\n",
    "        Args:\n",
    "            weights_path (str): Path to the pretrained weights file.\n",
    "            **kwargs: Additional arguments to pass to the constructor.\n",
    "\n",
    "        Returns:\n",
    "            UNet: Initialized UNet model with pretrained weights.\n",
    "        \"\"\"\n",
    "        model = cls(**kwargs)\n",
    "        model.load_weights(weights_path)\n",
    "        return model\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        skip_connections = []\n",
    "\n",
    "        for down in self.downs:\n",
    "            x = down(x)\n",
    "            skip_connections.append(x)\n",
    "            x = self.pool(x)\n",
    "\n",
    "        x = self.bottleneck(x)\n",
    "        skip_connections = skip_connections[::-1]\n",
    "\n",
    "        for idx in range(0, len(self.ups), 2):\n",
    "            x = self.ups[idx](x)\n",
    "            skip_connection = skip_connections[idx//2]\n",
    "\n",
    "            if x.shape != skip_connection.shape:\n",
    "                x = TF.resize(x, size=skip_connection.shape[2:])\n",
    "\n",
    "            concat_skip = torch.cat((skip_connection, x), dim=1)\n",
    "            x = self.ups[idx+1](concat_skip)\n",
    "        \n",
    "#        x = torch.sigmoid(x)\n",
    "\n",
    "        return self.final_conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "model_path = \"../data/models/unet_model_test_1_best_model.pth\"\n",
    "model = torch.load(model_path)\n",
    "\n",
    "model = UNET()\n",
    "\n",
    "\n",
    "# Load the state dictionary into the model\n",
    "model.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "# Chop the image into tiles\n",
    "adapter = ImageAdapter(img, patch_size=256, overlap_ratio=0.0)\n",
    "#adapter.clear_background()\n",
    "image_patches = adapter.create_patches()\n",
    "normalized_images = np.empty_like(image_patches, dtype=np.float32)\n",
    "\n",
    "for i in range(image_patches.shape[0]):\n",
    "    normalized_images[i] = normalize_percentile(image_patches[i])\n",
    "image_patches=normalized_images\n",
    "# Transform the image patches into tensors\n",
    "transform = transforms.ToTensor()\n",
    "tensor_patches = [transform(patch) for patch in image_patches]\n",
    "\n",
    "# Convert the tensor patches into a batch tensor\n",
    "batch_tensor = torch.stack(tensor_patches)\n",
    "\n",
    "# Move to the device and predict\n",
    "batch_tensor = batch_tensor.to(device)\n",
    "model = model.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    predictions = model(batch_tensor)\n",
    "\n",
    "# Convert the predictions back to numpy array\n",
    "predictions = predictions.cpu()\n",
    "\n",
    "# Stitch back the image\n",
    "reconstructed_image = adapter.stich_patches(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a figure with a specific size (width, height)\n",
    "fig = plt.figure(figsize=(12, 8))  # Adjust the width and height as needed\n",
    "\n",
    "plt.imshow(2**reconstructed_image, cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reconstructed_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reconstructed_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
